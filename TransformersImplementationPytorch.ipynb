{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPF05jxzbMfms9Ybi7DUo62",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AzeemWaqarRao/Pytorch_Implementations/blob/main/TransformersImplementationPytorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# importing required libraries\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import math,copy,re\n",
        "import warnings\n",
        "import random\n",
        "from __future__ import unicode_literals, print_function, division\n",
        "from io import open\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import unicodedata\n",
        "import seaborn as sns\n",
        "import torchtext\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler\n",
        "\n",
        "warnings.simplefilter(\"ignore\")\n",
        "print(torch.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JPvs_u-gVgum",
        "outputId": "d995c7e4-0a60-4279-cc32-c6645f8401ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.0.1+cu118\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NAujPUEEd9bd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# helper functions\n",
        "\n",
        "def unicodeToAscii(s):\n",
        "    return ''.join(\n",
        "        c for c in unicodedata.normalize('NFD', s)\n",
        "        if unicodedata.category(c) != 'Mn'\n",
        "    )\n",
        "\n",
        "# Lowercase, trim, and remove non-letter characters\n",
        "def normalizeString(s):\n",
        "    s = unicodeToAscii(s.lower().strip())\n",
        "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
        "    s = re.sub(r\"[^a-zA-Z!?]+\", r\" \", s)\n",
        "    return s.strip()\n",
        "\n",
        "\n",
        "# reading data from file\n",
        "def filter_pairs(pairs, MAXLENGTH):\n",
        "  new_pairs = []\n",
        "  for pair in pairs:\n",
        "    if len(pair[0].split(' ')) <=MAXLENGTH and len(pair[1].split(' ')) <=MAXLENGTH:\n",
        "      new_pairs.append(pair)\n",
        "  return new_pairs\n",
        "\n",
        "\n",
        "def read_data(path,lang1,lang2,MAXLENGTH):\n",
        "  with open(path, 'r') as f:\n",
        "    lines = f.read().strip().split('\\n')\n",
        "    lines = [[normalizeString(sent.lower()) for sent in line.split('\\t')] for line in lines]\n",
        "\n",
        "  input_lang = Lang(lang1)\n",
        "  output_lang = Lang(lang2)\n",
        "\n",
        "  for line in lines:\n",
        "    input_lang.addSentence(line[0])\n",
        "    output_lang.addSentence(line[1])\n",
        "\n",
        "\n",
        "  lines = filter_pairs(lines,MAXLENGTH)\n",
        "\n",
        "  return input_lang, output_lang, lines\n",
        "\n",
        "\n",
        "def padd_seq(seq,length,place=0):\n",
        "  if len(seq) < length:\n",
        "    for i in range(length-len(seq)):\n",
        "      seq.append(0)\n",
        "  return seq\n",
        "\n",
        "def sent_to_index(lines,input_lang,output_lang,MAXLENGTH):\n",
        "  inputs = []\n",
        "  targets = []\n",
        "  for line in lines:\n",
        "    inp = padd_seq(input_lang.sent_to_index(line[0]),MAXLENGTH+1)\n",
        "    targ = padd_seq(output_lang.sent_to_index(line[1]),MAXLENGTH+1)\n",
        "    inputs.append(inp)\n",
        "    targets.append(targ)\n",
        "\n",
        "  return inputs, targets\n",
        "\n",
        "\n",
        "# gives us a data loader with inputs and targets\n",
        "\n",
        "def get_dataloader(inputs,outputs,batch_size,device):\n",
        "  train_data = TensorDataset(torch.tensor(inputs,dtype=torch.long, device=device),\n",
        "                               torch.tensor(targets,dtype=torch.long,device=device))\n",
        "  train_data = DataLoader(train_data,batch_size=batch_size)\n",
        "  return train_data"
      ],
      "metadata": {
        "id": "28fTNzUYd9d2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SOS_token = 0\n",
        "EOS_token = 1\n",
        "\n",
        "class Lang:\n",
        "    def __init__(self, name):\n",
        "        self.name = name\n",
        "        self.word2index = {'SOS':0 , 'EOS':1}\n",
        "        self.word2count = {}\n",
        "        self.index2word = {0: \"SOS\", 1: \"EOS\"}\n",
        "        self.n_words = 2  # Count SOS and EOS\n",
        "\n",
        "    def addSentence(self, sentence):\n",
        "        for word in sentence.split(' '):\n",
        "            self.addWord(word)\n",
        "\n",
        "    def addWord(self, word):\n",
        "        if word not in self.word2index:\n",
        "            self.word2index[word] = self.n_words\n",
        "            self.word2count[word] = 1\n",
        "            self.index2word[self.n_words] = word\n",
        "            self.n_words += 1\n",
        "        else:\n",
        "            self.word2count[word] += 1\n",
        "\n",
        "    def sent_to_index(self,sent):\n",
        "      sent =  [self.word2index[word] for word in sent.split(' ')]\n",
        "      sent.append(EOS_token)\n",
        "      return sent"
      ],
      "metadata": {
        "id": "zQkCxq5BeJco"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "R2DGnKu5eJfD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2c4GTf4feJqu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Creates embeddings for input sequences\n",
        "(*) -> (*, H)\n",
        "where H is dimension size\n",
        "\"\"\"\n",
        "class Embeddings(nn.Module):\n",
        "  def __init__(self, vocab_size, encode_dim):\n",
        "    super(Embeddings, self).__init__()\n",
        "    self.vocab_size = vocab_size\n",
        "    self.encode_dim = encode_dim\n",
        "\n",
        "    self.embed = nn.Embedding(self.vocab_size, self.encode_dim)\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.embed(x)"
      ],
      "metadata": {
        "id": "lJanbHXCnEt4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# class PositionalEncoding(nn.Module):\n",
        "#   def __init__(self, seq_len, embed_dim):\n",
        "#     super(PositionalEncoding, self).__init__()\n",
        "\n",
        "#     self.embed_dim = embed_dim\n",
        "#     pe = torch.zeros(seq_len, self.embed_dim)\n",
        "#     for pos in range(seq_len):\n",
        "#       for i in range(0, self.embed_dim, 2):\n",
        "#         pe[pos,i] = math.sin(pos / (10000 ** ((2 * i)/self.embed_dim)))\n",
        "#         pe[pos,i+1] = math.cos(pos / (10000 ** ((2 * (i + 1))/self.embed_dim)))\n",
        "\n",
        "#     pe = pe.unsqueeze(0)\n",
        "#     self.register_buffer('pe', pe)\n",
        "\n",
        "#   def forward(self, x):\n",
        "#     x = x * math.sqrt(self.embed_dim)\n",
        "#     seq_len = x.size(1)\n",
        "#     x_new = x[:,:-1] + torch.autograd.Variable(self.pe[:,:seq_len], requires_grad=False)\n",
        "#     x = torch.cat([x_new,x[:,-1].unsqueeze(dim=1)],dim=1)\n",
        "#     return x\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "qVAYdjwho171"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SelfAttention(nn.Module):\n",
        "  def __init__(self, n_heads, embed_dim):\n",
        "    super(SelfAttention, self).__init__()\n",
        "    self.n_heads = n_heads\n",
        "    self.embed_dim = embed_dim\n",
        "\n",
        "    self.head_dim = int(self.embed_dim / self.n_heads)\n",
        "\n",
        "    self.k_mat = nn.Linear(self.head_dim, self.head_dim, bias = False)\n",
        "    self.v_mat = nn.Linear(self.head_dim, self.head_dim, bias = False)\n",
        "    self.q_mat = nn.Linear(self.head_dim, self.head_dim, bias = False)\n",
        "    self.linear = nn.Linear(self.n_heads * self.head_dim, self.embed_dim)\n",
        "\n",
        "\n",
        "  def forward(self, key, query, val, mask=None):\n",
        "    self.batch_size = key.shape[0]\n",
        "    self.seq_len = key.shape[1]\n",
        "    self.seq_len_query = query.shape[1]\n",
        "\n",
        "    # old shape -> (32,10,512)\n",
        "    key = key.view(self.batch_size, self.seq_len, self.n_heads, self.head_dim)\n",
        "    query = query.view(self.batch_size, self.seq_len_query, self.n_heads, self.head_dim)\n",
        "    val = val.view(self.batch_size, self.seq_len, self.n_heads, self.head_dim)\n",
        "    # new shape -> (32,10,8,64)\n",
        "\n",
        "\n",
        "\n",
        "    k = self.k_mat(key)\n",
        "    q = self.q_mat(query)\n",
        "    v = self.v_mat(val)\n",
        "\n",
        "\n",
        "    k = k.permute(0,2,1,3)\n",
        "    q = q.permute(0,2,1,3)\n",
        "    v = v.permute(0,2,1,3)\n",
        "    # new shape -> (32,8,10,64) ,, for matrix multiplication\n",
        "\n",
        "    scores = torch.matmul(q, k.permute(0,1,3,2))\n",
        "    # k will become -> (32,8,64,10)\n",
        "    # scores will be -> (32,8,10,10)\n",
        "    # fill those positions of scores matrix as (-1e20) where mask positions are 0\n",
        "    if mask is not None:\n",
        "      scores = scores.masked_fill(mask == 0, float(\"-1e20\"))\n",
        "\n",
        "\n",
        "    scores = scores / math.sqrt(self.head_dim) # 8\n",
        "\n",
        "    scores = F.log_softmax(scores, dim=-1)\n",
        "\n",
        "    z = torch.matmul(scores, v)\n",
        "    # z -> (32,8,10,64)\n",
        "\n",
        "    z = z.permute(0,2,1,3)\n",
        "    # z -> (32,10,8,64)\n",
        "\n",
        "    z = z.contiguous().view(self.batch_size, self.seq_len_query, self.n_heads * self.head_dim)\n",
        "    # z -> (32,10,512)\n",
        "\n",
        "    return self.linear(z)\n"
      ],
      "metadata": {
        "id": "eHa3C_fXt6lx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YE2PYLuc7oce"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderBlock(nn.Module):\n",
        "  def __init__(self, n_heads, embed_dim, expansion):\n",
        "    super(EncoderBlock, self).__init__()\n",
        "    self.n_heads = n_heads\n",
        "    self.embed_dim = embed_dim\n",
        "\n",
        "\n",
        "    self.attention = SelfAttention(self.n_heads, self.embed_dim)\n",
        "    self.ffnn = nn.Sequential(\n",
        "        nn.Linear(self.embed_dim, self.embed_dim * expansion),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(self.embed_dim * expansion, self.embed_dim)\n",
        "    )\n",
        "\n",
        "    self.norm1 = nn.LayerNorm(self.embed_dim)\n",
        "    self.norm2 = nn.LayerNorm(self.embed_dim)\n",
        "\n",
        "    self.dropout1 = nn.Dropout(0.2)\n",
        "    self.dropout2 = nn.Dropout(0.2)\n",
        "\n",
        "\n",
        "  def forward(self, key, query, val):\n",
        "    z = self.attention(key, query, val)\n",
        "    z_norm = self.dropout1(self.norm1(z+val))\n",
        "    # applying residual connection and normalization\n",
        "\n",
        "\n",
        "    out = self.ffnn(z_norm)\n",
        "    out_norm = self.dropout2(self.norm2(out + z_norm))\n",
        "    # applying residual connection and normalization\n",
        "\n",
        "    return out_norm\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "  def __init__(self, vocab_size, embed_dim, seq_len, n_heads = 8, expansion = 4, num_layers = 4):\n",
        "    super(Encoder, self).__init__()\n",
        "\n",
        "    self.num_layers = num_layers\n",
        "    self.embedding = Embeddings(vocab_size,embed_dim)\n",
        "    # self.poe = PositionalEncoding(seq_len, embed_dim)\n",
        "    self.poe = Embeddings(seq_len+1, embed_dim)\n",
        "\n",
        "    self.layers = nn.ModuleList([EncoderBlock(n_heads,embed_dim,expansion) for _ in range(self.num_layers)])\n",
        "\n",
        "  def forward(self, x):\n",
        "    # x -> (32,10)\n",
        "    batch_size, seq_len = x.shape\n",
        "    positions = torch.arange(0, seq_len).expand(batch_size, seq_len).to(device)\n",
        "\n",
        "    x = self.embedding(x) + self.poe(positions)\n",
        "    # x -> (32,10,512)\n",
        "    # x = self.poe(x)\n",
        "    for layer in self.layers:\n",
        "      x = layer(x, x, x)\n",
        "\n",
        "    return x # (32,10,512)"
      ],
      "metadata": {
        "id": "YCWW8NZYCY7A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# encoder = Encoder(1100, 512, 10, 8, 4, 2)\n",
        "# import numpy\n",
        "# arr = np.random.randint(0,1100,32*10).reshape(32,10)\n",
        "# arr = torch.tensor(arr)\n",
        "# encoder(arr).shape"
      ],
      "metadata": {
        "id": "gGztWgjgINis"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderBlock(nn.Module):\n",
        "  def __init__(self, n_heads, embed_dim, expansion):\n",
        "    super(DecoderBlock, self).__init__()\n",
        "    self.attention = SelfAttention(n_heads, embed_dim)\n",
        "    self.ed_attention = SelfAttention(n_heads, embed_dim)\n",
        "    self.ffnn = nn.Sequential(\n",
        "        nn.Linear(embed_dim, embed_dim*expansion),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(embed_dim*expansion, embed_dim)\n",
        "    )\n",
        "\n",
        "    self.dropout1 = nn.Dropout(0.2)\n",
        "    self.dropout2 = nn.Dropout(0.2)\n",
        "    self.dropout3 = nn.Dropout(0.2)\n",
        "\n",
        "    self.norm1 = nn.LayerNorm(embed_dim)\n",
        "    self.norm2 = nn.LayerNorm(embed_dim)\n",
        "    self.norm3 = nn.LayerNorm(embed_dim)\n",
        "\n",
        "  def forward(self, key, query, val, mask):\n",
        "\n",
        "    attention = self.attention(query, query, query, mask)\n",
        "    attention_out = self.dropout1(self.norm1(attention + query))\n",
        "\n",
        "\n",
        "\n",
        "    ed_attention = self.ed_attention(key, attention_out, val= key)\n",
        "    ed_attention_norm = self.dropout2(self.norm2(ed_attention + attention_out))\n",
        "\n",
        "    output = self.ffnn(ed_attention_norm)\n",
        "    output_norm = self.dropout3(self.norm3(output + ed_attention_norm))\n",
        "    return output_norm\n",
        "\n",
        "\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "  def __init__(self, embed_dim, vocab_size, seq_len, expansion=4, num_layers=2, n_heads=8):\n",
        "    super(Decoder, self).__init__()\n",
        "\n",
        "    # self.poe = PositionalEncoding(seq_len, embed_dim)\n",
        "    self.poe = Embeddings(seq_len+1, embed_dim)\n",
        "    self.embedding = Embeddings(vocab_size, embed_dim)\n",
        "    self.layers = nn.ModuleList([DecoderBlock(n_heads, embed_dim, expansion) for _ in range(num_layers)])\n",
        "    self.fc = nn.Linear(embed_dim, vocab_size)\n",
        "    self.dropout = nn.Dropout(0.2)\n",
        "\n",
        "  def forward(self, encoder_output, x, mask):\n",
        "\n",
        "    # x -> 32, 10\n",
        "    batch_size, seq_len = x.shape\n",
        "    positions = torch.arange(0, seq_len).expand(batch_size, seq_len).to(device)\n",
        "\n",
        "    x = self.embedding(x) + self.poe(positions)\n",
        "    # x -> 32, 10, 512\n",
        "    # x = self.poe(x)\n",
        "\n",
        "    x = self.dropout(x)\n",
        "\n",
        "    for layer in self.layers:\n",
        "      x = layer(encoder_output, x, encoder_output, mask)\n",
        "\n",
        "    output = F.softmax(self.fc(x))\n",
        "\n",
        "    return output"
      ],
      "metadata": {
        "id": "Jl9aX0XmKIKg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# arr = torch.tensor(np.random.randint(0,1100,32*1).reshape(32,1), dtype=torch.long)\n",
        "# enc = torch.tensor(np.random.randint(0,1100,32*10*512).reshape(32,10,512), dtype=torch.float)\n",
        "# decoder = Decoder(512,1100,10,4,2,8)"
      ],
      "metadata": {
        "id": "86keIl1HnS-l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# decoder(enc, arr,None).shape"
      ],
      "metadata": {
        "id": "_a8_0uCLmght"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AMRwolVznCW5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Transformer(nn.Module):\n",
        "  def __init__(self, encoder, decoder, embed_dim, seq_len, device):\n",
        "    super(Transformer, self).__init__()\n",
        "    self.seq_len = seq_len\n",
        "    self.embed_dim = embed_dim\n",
        "    self.encoder = encoder\n",
        "    self.decoder = decoder\n",
        "    self.device = device\n",
        "\n",
        "  def forward(self,src,trg):\n",
        "    trg_mask = self.create_mask(trg)\n",
        "    enc_out = self.encoder(src)\n",
        "    output = self.decoder(enc_out, trg, trg_mask)\n",
        "\n",
        "    return output\n",
        "\n",
        "\n",
        "  def decode(self,src, trg):\n",
        "    mask = self.create_mask(trg)\n",
        "    enc_out = self.encoder(src)\n",
        "    out_labels = []\n",
        "    batch_size, seq_len = src.shape[0], src.shape[1]\n",
        "    out = trg\n",
        "    for i in range(seq_len):\n",
        "      out = self.decoder(enc_out, out, mask)\n",
        "      out_labels.append(out[:,-1, :].unsqueeze(1))\n",
        "      _, out = out.topk(1)\n",
        "      out = out.squeeze(-1)\n",
        "      out = out.long()\n",
        "\n",
        "    out_labels = torch.cat(out_labels, dim=1)\n",
        "\n",
        "    return (out_labels)\n",
        "\n",
        "\n",
        "  def create_mask(self, trg):\n",
        "    batch_size, trg_len = trg.shape\n",
        "    # returns the lower triangular part of matrix filled with ones\n",
        "    trg_mask = torch.tril(torch.ones((trg_len, trg_len))).expand(\n",
        "        batch_size, 1, trg_len, trg_len\n",
        "    )\n",
        "    return trg_mask.to(self.device)"
      ],
      "metadata": {
        "id": "vE2VQCTZE-zT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# t = Transformer(1100,1100,512,10,8,4,4)\n",
        "# arr = torch.tensor(np.random.randint(0,1100,32*10).reshape(32,10), dtype=torch.long)\n",
        "# words = t(arr, arr)\n",
        "# words.shape"
      ],
      "metadata": {
        "id": "NHOjkY-eH3s1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# item = t.decode(arr,arr)\n",
        "# item.shape"
      ],
      "metadata": {
        "id": "hIA5m75vEB5X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gdMp5OOXuahg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hidden_size = 512\n",
        "lr = 0.0001\n",
        "epochs = 5\n",
        "batch_size = 32\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "path = '/content/eng-fra.txt'\n",
        "MAXLENGTH = 10\n",
        "num_layers = 2\n",
        "expansion = 4\n",
        "n_heads = 8\n",
        "\n",
        "\n",
        "input_lang, output_lang, lines = read_data(path,'English', 'French',MAXLENGTH)\n",
        "\n",
        "encoder = Encoder(input_lang.n_words,hidden_size, MAXLENGTH,n_heads, expansion, num_layers).to(device)\n",
        "decoder = Decoder(hidden_size, output_lang.n_words, MAXLENGTH, expansion, num_layers, n_heads).to(device)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "inputs, targets = sent_to_index(lines,input_lang, output_lang,MAXLENGTH)\n",
        "data_loader = get_dataloader(inputs,targets,batch_size, device)\n",
        "# _,_,data_loader = get_dataloader(batch_size)\n",
        "\n",
        "num_batches = len(data_loader)\n",
        "\n",
        "transformer = Transformer(encoder, decoder, hidden_size, MAXLENGTH, device)\n",
        "\n",
        "optimizer = torch.optim.Adam(transformer.parameters(), lr)\n",
        "\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "z4ZXmZVBXa8T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# arr = torch.tensor(np.random.randint(0,1100,1*11).reshape(1,11), dtype=torch.long).to(device)\n"
      ],
      "metadata": {
        "id": "GK-VtSM5uILE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# transformer.decode(arr, arr).shape"
      ],
      "metadata": {
        "id": "7Rclc4tNuIBp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(1):\n",
        "  print(f\"Epoch {epoch+1} starting\")\n",
        "  total_loss = 0\n",
        "  for batch in data_loader:\n",
        "    input, target = batch\n",
        "\n",
        "    output = transformer(input, target)\n",
        "\n",
        "    loss = loss_fn(\n",
        "            output.view(-1, output.size(-1)),\n",
        "            target.view(-1)\n",
        "        )\n",
        "    encoder_optimizer.zero_grad()\n",
        "    decoder_optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    encoder_optimizer.step()\n",
        "    decoder_optimizer.step()\n",
        "\n",
        "    total_loss += loss.item()\n",
        "\n",
        "  print(f\"Loss : {total_loss/num_batches}\")\n",
        "  print(f\"Epoch {epoch+1} ended\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JuLtyKhaXa-r",
        "outputId": "97eefaf4-e4b4-450e-a2c9-6388ce1562e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 starting\n",
            "Loss : -0.03128795288554767\n",
            "Epoch 1 ended\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluateRandomly(transformer, n=10):\n",
        "    for i in range(n):\n",
        "        pair = random.choice(lines)\n",
        "        print('>', pair[0])\n",
        "        print('=', pair[1])\n",
        "        output_words = evaluate(transformer, pair, input_lang, output_lang)\n",
        "        output_sentence = ' '.join(output_words)\n",
        "        print('<', output_sentence)\n",
        "        print('')\n"
      ],
      "metadata": {
        "id": "-a-LWBjJMjcf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(transformer, pair, input_lang, output_lang):\n",
        "    with torch.no_grad():\n",
        "        sentence = pair[0]\n",
        "        target = pair[1]\n",
        "        input_tensor = input_lang.sent_to_index(sentence)\n",
        "        input_tensor = padd_seq(input_tensor,MAXLENGTH+1)\n",
        "\n",
        "        input_tensor = torch.LongTensor(input_tensor).view(1,-1).to(device)\n",
        "\n",
        "        target_tensor = output_lang.sent_to_index(target)\n",
        "        target_tensor = padd_seq(target_tensor,MAXLENGTH+1)\n",
        "\n",
        "        target_tensor = torch.LongTensor(target_tensor).view(1,-1).to(device)\n",
        "\n",
        "        outputs = transformer.decode(input_tensor, target_tensor)\n",
        "\n",
        "        _, topi = outputs.topk(1)\n",
        "        decoded_ids = topi.squeeze()\n",
        "\n",
        "        decoded_words = []\n",
        "        for idx in decoded_ids:\n",
        "            if idx.item() == EOS_token:\n",
        "                decoded_words.append('<EOS>')\n",
        "                break\n",
        "            decoded_words.append(output_lang.index2word[idx.item()])\n",
        "    return decoded_words"
      ],
      "metadata": {
        "id": "bLKJwXNGNx-v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transformer.eval()\n",
        "evaluateRandomly(transformer)"
      ],
      "metadata": {
        "id": "fp2PqPyNN2il",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6dc8a6fd-a04c-45af-a1cf-f3541fa3b0ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "> he died yesterday\n",
            "= il a clamece hier\n",
            "< SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS\n",
            "\n",
            "> what s the name of your insurance company ?\n",
            "= quel est le nom de ta compagnie d assurance ?\n",
            "< SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS\n",
            "\n",
            "> he never cared much for me\n",
            "= il ne s est jamais beaucoup preoccupe de moi\n",
            "< SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS\n",
            "\n",
            "> don t tell my girlfriend\n",
            "= ne dites rien a ma petite amie !\n",
            "< SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS\n",
            "\n",
            "> i asked for a seat in the no smoking section\n",
            "= j ai demande une place en non fumeur\n",
            "< SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS\n",
            "\n",
            "> please eat some cake\n",
            "= s il te plait mange un peu de gateau !\n",
            "< SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS\n",
            "\n",
            "> i think it s time for me to shove off\n",
            "= je pense qu il est temps pour moi de partir\n",
            "< SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS\n",
            "\n",
            "> they lie all the time\n",
            "= elles mentent tout le temps\n",
            "< SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS\n",
            "\n",
            "> can t we keep this between us ?\n",
            "= ne pouvons nous pas garder ca entre nous ?\n",
            "< SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS\n",
            "\n",
            "> do you go to school by bus ?\n",
            "= vous rendez vous a l ecole en bus ?\n",
            "< SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qB8DApil9mkk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}