{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMRN711PkZ8eI1HgTNSwnym",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AzeemWaqarRao/Pytorch_Implementations/blob/main/seq2seq_implementation_pytorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "buafjabeFhsg"
      },
      "outputs": [],
      "source": [
        "from __future__ import unicode_literals, print_function, division\n",
        "from io import open\n",
        "import unicodedata\n",
        "import re\n",
        "import random\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import numpy as np\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# helper functions\n",
        "\n",
        "def unicodeToAscii(s):\n",
        "    return ''.join(\n",
        "        c for c in unicodedata.normalize('NFD', s)\n",
        "        if unicodedata.category(c) != 'Mn'\n",
        "    )\n",
        "\n",
        "# Lowercase, trim, and remove non-letter characters\n",
        "def normalizeString(s):\n",
        "    s = unicodeToAscii(s.lower().strip())\n",
        "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
        "    s = re.sub(r\"[^a-zA-Z!?]+\", r\" \", s)\n",
        "    return s.strip()\n",
        "\n",
        "\n",
        "# reading data from file\n",
        "def filter_pairs(pairs, MAXLENGTH):\n",
        "  new_pairs = []\n",
        "  for pair in pairs:\n",
        "    if len(pair[0].split(' ')) <=MAXLENGTH and len(pair[1].split(' ')) <=MAXLENGTH:\n",
        "      new_pairs.append(pair)\n",
        "  return new_pairs\n",
        "\n",
        "\n",
        "def read_data(path,lang1,lang2,MAXLENGTH):\n",
        "  with open(path, 'r') as f:\n",
        "    lines = f.read().strip().split('\\n')\n",
        "    lines = [[normalizeString(sent.lower()) for sent in line.split('\\t')] for line in lines]\n",
        "\n",
        "  input_lang = Lang(lang1)\n",
        "  output_lang = Lang(lang2)\n",
        "\n",
        "  for line in lines:\n",
        "    input_lang.addSentence(line[0])\n",
        "    output_lang.addSentence(line[1])\n",
        "\n",
        "\n",
        "  lines = filter_pairs(lines,MAXLENGTH)\n",
        "\n",
        "  return input_lang, output_lang, lines\n",
        "\n",
        "\n",
        "def padd_seq(seq,length,place=0):\n",
        "  if len(seq) < length:\n",
        "    for i in range(length-len(seq)):\n",
        "      seq.append(0)\n",
        "  return seq\n",
        "\n",
        "def sent_to_index(lines,input_lang,output_lang,MAXLENGTH):\n",
        "  inputs = []\n",
        "  targets = []\n",
        "  for line in lines:\n",
        "    inp = padd_seq(input_lang.sent_to_index(line[0]),MAXLENGTH+1)\n",
        "    targ = padd_seq(output_lang.sent_to_index(line[1]),MAXLENGTH+1)\n",
        "    inputs.append(inp)\n",
        "    targets.append(targ)\n",
        "\n",
        "  return inputs, targets\n",
        "\n",
        "\n",
        "# gives us a data loader with inputs and targets\n",
        "\n",
        "def get_dataloader(inputs,outputs,batch_size,device):\n",
        "  train_data = TensorDataset(torch.tensor(inputs,dtype=torch.long, device=device),\n",
        "                               torch.tensor(targets,dtype=torch.long,device=device))\n",
        "  train_data = DataLoader(train_data,batch_size=batch_size)\n",
        "  return train_data"
      ],
      "metadata": {
        "id": "Kag8fqyhGYBM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SOS_token = 0\n",
        "EOS_token = 1\n",
        "\n",
        "class Lang:\n",
        "    def __init__(self, name):\n",
        "        self.name = name\n",
        "        self.word2index = {'SOS':0 , 'EOS':1}\n",
        "        self.word2count = {}\n",
        "        self.index2word = {0: \"SOS\", 1: \"EOS\"}\n",
        "        self.n_words = 2  # Count SOS and EOS\n",
        "\n",
        "    def addSentence(self, sentence):\n",
        "        for word in sentence.split(' '):\n",
        "            self.addWord(word)\n",
        "\n",
        "    def addWord(self, word):\n",
        "        if word not in self.word2index:\n",
        "            self.word2index[word] = self.n_words\n",
        "            self.word2count[word] = 1\n",
        "            self.index2word[self.n_words] = word\n",
        "            self.n_words += 1\n",
        "        else:\n",
        "            self.word2count[word] += 1\n",
        "\n",
        "    def sent_to_index(self,sent):\n",
        "      sent =  [self.word2index[word] for word in sent.split(' ')]\n",
        "      sent.append(EOS_token)\n",
        "      return sent"
      ],
      "metadata": {
        "id": "Qg6FwsFZIBh7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# path = '/content/eng-fra.txt'\n",
        "# input_lang, output_lang, lines = read_data(path,'English', 'French',10)\n",
        "# print(f\"number of unique {input_lang.name} words : {input_lang.n_words}\")\n",
        "# print(f\"Number of unique {output_lang.name} words : {output_lang.n_words}\")\n",
        "# print(f\"Dimension of Sentence Pairs {len(lines), len(lines[0])}\")"
      ],
      "metadata": {
        "id": "F0wIrjnKHFL8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# lines"
      ],
      "metadata": {
        "id": "FJZ0mPFvDIXZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Encoder\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "  def __init__(self, num_words, vec_size, MAXLENGTH, dp=0.2):\n",
        "    super(Encoder, self).__init__()\n",
        "    self.num_words = num_words\n",
        "    self.vec_size = vec_size\n",
        "    self.maxlength = MAXLENGTH\n",
        "\n",
        "    self.embedding = nn.Embedding(self.num_words, self.vec_size)\n",
        "    self.gru = nn.GRU(self.vec_size, self.vec_size,batch_first=True)\n",
        "    self.dropout = nn.Dropout(dp)\n",
        "\n",
        "  def forward(self,x):\n",
        "    outputs = []\n",
        "    hidden = torch.zeros(1,x.shape[0], self.vec_size, dtype=torch.float).to(device)\n",
        "\n",
        "    for i in range(self.maxlength+1):\n",
        "      input = x[:,i].unsqueeze(1)\n",
        "      output, hidden = self.forward_step(input, hidden)\n",
        "      outputs.append(output)\n",
        "\n",
        "    outputs = torch.cat(outputs, dim=1)\n",
        "    return outputs, hidden\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  def forward_step(self,x, hidden):\n",
        "    # x -> (*) tensor size\n",
        "\n",
        "    embeddings = self.dropout(self.embedding(x))\n",
        "    # embeddings -> (* , H) H = hidden_size\n",
        "\n",
        "    output, hidden = self.gru(embeddings, hidden)\n",
        "\n",
        "    # output -> (batch, seq_lenght, H_inp)\n",
        "    # hidden -> (1, batch, H_out)\n",
        "\n",
        "    return output, hidden"
      ],
      "metadata": {
        "id": "dIT5U57vX8Sc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(nn.Module):\n",
        "  def __init__(self,num_words,vec_size, MAXLENGTH):\n",
        "    super(Decoder, self).__init__()\n",
        "\n",
        "    self.num_words = num_words\n",
        "    self.vec_size = vec_size\n",
        "    self.max_length = MAXLENGTH\n",
        "\n",
        "    self.embedding = nn.Embedding(self.num_words,self.vec_size)\n",
        "    self.gru = nn.GRU(self.vec_size, self.vec_size,batch_first=True)\n",
        "    self.linear = nn.Linear(self.vec_size, self.num_words)\n",
        "\n",
        "  def forward(self,encoder_output, encoder_hidden):\n",
        "    batch_size = encoder_output.size(0)\n",
        "    decoder_input = torch.zeros(batch_size,1,dtype=torch.long,device=device)\n",
        "    hidden = encoder_hidden\n",
        "\n",
        "    outputs = []\n",
        "\n",
        "    for i in range(self.max_length+1):\n",
        "      output, hidden = self.forward_step(decoder_input, hidden)\n",
        "      outputs.append(output)\n",
        "\n",
        "      _, topi = output.topk(1)\n",
        "      decoder_input = topi.squeeze(-1).detach()\n",
        "\n",
        "\n",
        "\n",
        "    outputs = torch.cat(outputs,dim=1)\n",
        "    outputs = F.log_softmax(outputs,dim=-1)\n",
        "\n",
        "    return outputs, hidden\n",
        "\n",
        "  def forward_step(self, input, hidden_state):\n",
        "\n",
        "    input = self.embedding(input)\n",
        "    input = F.relu(input)\n",
        "    output, hidden_state = self.gru(input, hidden_state)\n",
        "    output = self.linear(output)\n",
        "    return output, hidden_state\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "3q0hGFdAbdUx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Attention(nn.Module):\n",
        "  def __init__(self,hidden_size):\n",
        "    super(Attention, self).__init__()\n",
        "    self.linear_q = nn.Linear(hidden_size, hidden_size)\n",
        "    self.linear_k = nn.Linear(hidden_size, hidden_size)\n",
        "    self.linear = nn.Linear(hidden_size,1)\n",
        "\n",
        "  def forward(self, query, keys):\n",
        "    val = self.linear(torch.tanh(self.linear_q(query) + self.linear_k(keys)))\n",
        "    weights = val.permute(0,2,1)\n",
        "    weights = F.log_softmax(weights,-1)\n",
        "\n",
        "    context = torch.bmm(weights, keys)\n",
        "\n",
        "    return context, weights\n",
        "\n",
        "\n",
        "\n",
        "class AttentionDecoder(nn.Module):\n",
        "  def __init__(self, output_size, hidden_size, MAXLENGTH, dp=0.1):\n",
        "    super(AttentionDecoder,self).__init__()\n",
        "    self.attention = Attention(hidden_size)\n",
        "    self.embedding = nn.Embedding(output_size, hidden_size)\n",
        "    self.dropout = nn.Dropout(dp)\n",
        "    self.gru = nn.GRU(2*hidden_size, hidden_size, batch_first = True)\n",
        "    self.linear = nn.Linear(hidden_size, output_size)\n",
        "    self.maxlength = MAXLENGTH\n",
        "\n",
        "  def forward(self, encoder_output, encoder_hidden, target=None):\n",
        "    batch_size = encoder_output.shape[0]\n",
        "    inp = torch.zeros(batch_size, 1, dtype = torch.long, device = device)\n",
        "    hidden = encoder_hidden\n",
        "    outputs = []\n",
        "    attentions = []\n",
        "    for i in range(self.maxlength + 1):\n",
        "      output, hidden, weights = self.forward_step(inp, hidden, encoder_output)\n",
        "      outputs.append(output)\n",
        "      attentions.append(weights)\n",
        "\n",
        "      if target is not None:\n",
        "        inp = target[:,i].unsqueeze(1)\n",
        "\n",
        "      else:\n",
        "        _, topi = output.topk(1)\n",
        "        inp = topi.squeeze(-1).detach()\n",
        "\n",
        "\n",
        "    outputs = torch.cat(outputs,dim=1)\n",
        "    outputs = F.log_softmax(outputs,dim=-1)\n",
        "    attentions = torch.cat(attentions,dim=1)\n",
        "\n",
        "    return outputs, hidden, attentions\n",
        "\n",
        "\n",
        "  def forward_step(self, inp, hidden, encoder_output):\n",
        "    inp = self.dropout(self.embedding(inp))\n",
        "\n",
        "    context, weights = self.attention(hidden.permute(1,0,2), encoder_output)\n",
        "\n",
        "    input = torch.cat([inp,context], dim=2)\n",
        "\n",
        "    output, hidden = self.gru(input,hidden)\n",
        "    output = self.linear(output)\n",
        "    return output, hidden, weights\n"
      ],
      "metadata": {
        "id": "beewysBW5H8Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hidden_size = 128\n",
        "lr = 0.001\n",
        "epochs = 30\n",
        "batch_size = 32\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "# device = 'cpu'\n",
        "path = '/content/eng-fra.txt'\n",
        "MAXLENGTH = 20\n",
        "\n",
        "\n",
        "\n",
        "input_lang, output_lang, lines = read_data(path,'English', 'French',MAXLENGTH)\n",
        "\n",
        "encoder = Encoder(input_lang.n_words,hidden_size, MAXLENGTH).to(device)\n",
        "decoder = AttentionDecoder(output_lang.n_words, hidden_size, MAXLENGTH).to(device)\n",
        "\n",
        "encoder_optimizer = optim.Adam(encoder.parameters(), lr)\n",
        "decoder_optimizer = optim.Adam(decoder.parameters(), lr)\n",
        "\n",
        "\n",
        "inputs, targets = sent_to_index(lines,input_lang, output_lang,MAXLENGTH)\n",
        "data_loader = get_dataloader(inputs,targets,batch_size, device)\n",
        "# _,_,data_loader = get_dataloader(batch_size)\n",
        "\n",
        "num_batches = len(data_loader)\n",
        "\n",
        "loss_fn = nn.NLLLoss()"
      ],
      "metadata": {
        "id": "FsjviS61hCN-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(1):\n",
        "  print(f\"Epoch {epoch+1} starting\")\n",
        "  total_loss = 0\n",
        "  for batch in data_loader:\n",
        "    input, target = batch\n",
        "\n",
        "    output, hidden = encoder(input)\n",
        "    output, _, _ = decoder(output, hidden, target)\n",
        "\n",
        "    loss = loss_fn(\n",
        "            output.view(-1, output.size(-1)),\n",
        "            target.view(-1)\n",
        "        )\n",
        "    encoder_optimizer.zero_grad()\n",
        "    decoder_optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    encoder_optimizer.step()\n",
        "    decoder_optimizer.step()\n",
        "\n",
        "    total_loss += loss.item()\n",
        "\n",
        "  print(f\"Loss : {total_loss/num_batches}\")\n",
        "  print(f\"Epoch {epoch+1} ended\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S_JeVbEds3Oj",
        "outputId": "14f50947-f02f-41c3-e12d-46380595c62b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 starting\n",
            "Loss : 2.2477341580064327\n",
            "Epoch 1 ended\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4bw19QkhtwIZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluateRandomly(encoder, decoder, n=10):\n",
        "    for i in range(n):\n",
        "        pair = random.choice(lines)\n",
        "        print('>', pair[0])\n",
        "        print('=', pair[1])\n",
        "        output_words = evaluate(encoder, decoder, pair[0], input_lang, output_lang)\n",
        "        output_sentence = ' '.join(output_words)\n",
        "        print('<', output_sentence)\n",
        "        print('')\n"
      ],
      "metadata": {
        "id": "O9mDIUNRu0Tl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(encoder, decoder, sentence, input_lang, output_lang):\n",
        "    with torch.no_grad():\n",
        "        input_tensor = input_lang.sent_to_index(sentence)\n",
        "        input_tensor = padd_seq(input_tensor,11)\n",
        "\n",
        "        input_tensor = torch.LongTensor(input_tensor).view(1,-1).to(device)\n",
        "\n",
        "        encoder_outputs, encoder_hidden = encoder(input_tensor)\n",
        "        decoder_outputs, decoder_hidden, _ = decoder(encoder_outputs, encoder_hidden)\n",
        "\n",
        "        _, topi = decoder_outputs.topk(1)\n",
        "        decoded_ids = topi.squeeze()\n",
        "\n",
        "        decoded_words = []\n",
        "        for idx in decoded_ids:\n",
        "            if idx.item() == EOS_token:\n",
        "                decoded_words.append('<EOS>')\n",
        "                break\n",
        "            decoded_words.append(output_lang.index2word[idx.item()])\n",
        "    return decoded_words"
      ],
      "metadata": {
        "id": "Gkw08_kT5Z9l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder.eval()\n",
        "decoder.eval()\n",
        "evaluateRandomly(encoder, decoder)"
      ],
      "metadata": {
        "id": "JxBVZkq285sK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0Ew-S4b4m_zW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.zeros(1,32, 128, dtype=torch.long).shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OFestjWhNqz-",
        "outputId": "73c4d88e-bfcd-4649-df26-9fff02448708"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 32, 128])"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZxyqNdPe7aGZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}