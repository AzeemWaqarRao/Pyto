{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO1+K+EWRwPBEgaCyTchMqC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AzeemWaqarRao/Pytorch_Implementations/blob/main/LogisticRegressionPytorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uuw4M-nJQ2wv"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from sklearn import datasets\n",
        "from sklearn. preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# getting data\n",
        "dataset = datasets.load_breast_cancer()"
      ],
      "metadata": {
        "id": "N3iXLw9pRTLU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X,y = dataset.data , dataset.target"
      ],
      "metadata": {
        "id": "wTHK5ypTSBe8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_features = len(dataset.feature_names)"
      ],
      "metadata": {
        "id": "LN-AkDCqSIXd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)"
      ],
      "metadata": {
        "id": "U4-jM_SKSM99"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Scaling Data"
      ],
      "metadata": {
        "id": "kew8AE-RSfZw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scalar = StandardScaler()"
      ],
      "metadata": {
        "id": "H7x-3ISDSs3K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = scalar.fit_transform(X_train)\n",
        "X_test = scalar.transform(X_test)"
      ],
      "metadata": {
        "id": "OfuIHHJ4SxiT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# transforming data to tensors"
      ],
      "metadata": {
        "id": "X0kWvYt0azsx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = torch.from_numpy(X_train.astype(np.float32))\n",
        "X_test = torch.from_numpy(X_test.astype(np.float32))\n",
        "y_train = torch.from_numpy(y_train.astype(np.float32)).view(-1,1)\n",
        "y_test = torch.from_numpy(y_test.astype(np.float32)).view(-1,1)"
      ],
      "metadata": {
        "id": "nTHn_RaNS87c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model creation"
      ],
      "metadata": {
        "id": "CnzHzWy4aKip"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LogisticRegression(nn.Module):\n",
        "  def __init__(self,n_features):\n",
        "    super().__init__()\n",
        "    self.linear = nn.Linear(n_features,1)\n",
        "\n",
        "  def forward(self,x):\n",
        "    return torch.sigmoid(self.linear(x))"
      ],
      "metadata": {
        "id": "iIF8KKXUa7gD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = LogisticRegression(n_features)"
      ],
      "metadata": {
        "id": "Ttn4UO50bcYC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss_fn = nn.BCELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(),0.01)"
      ],
      "metadata": {
        "id": "7VYolP93bhBq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 100\n",
        "for epoch in range(epochs):\n",
        "  y_pred = model(X_train)\n",
        "  loss = loss_fn(y_pred,y_train)\n",
        "\n",
        "  optimizer.zero_grad()\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "  print(f\"epoch {epoch} , loss {loss}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FxBfmdWxbmqE",
        "outputId": "ac7ab5ba-d0e0-4711-e3e1-d2023ae8a8ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 0 , loss 0.0711342990398407\n",
            "epoch 1 , loss 0.07102185487747192\n",
            "epoch 2 , loss 0.07091037184000015\n",
            "epoch 3 , loss 0.0707997977733612\n",
            "epoch 4 , loss 0.0706901103258133\n",
            "epoch 5 , loss 0.0705813392996788\n",
            "epoch 6 , loss 0.07047343999147415\n",
            "epoch 7 , loss 0.07036643475294113\n",
            "epoch 8 , loss 0.07026026397943497\n",
            "epoch 9 , loss 0.07015497237443924\n",
            "epoch 10 , loss 0.07005047798156738\n",
            "epoch 11 , loss 0.06994685530662537\n",
            "epoch 12 , loss 0.06984403729438782\n",
            "epoch 13 , loss 0.06974203884601593\n",
            "epoch 14 , loss 0.06964083015918732\n",
            "epoch 15 , loss 0.06954041868448257\n",
            "epoch 16 , loss 0.0694408044219017\n",
            "epoch 17 , loss 0.06934194266796112\n",
            "epoch 18 , loss 0.06924386322498322\n",
            "epoch 19 , loss 0.0691465362906456\n",
            "epoch 20 , loss 0.06904993951320648\n",
            "epoch 21 , loss 0.06895408779382706\n",
            "epoch 22 , loss 0.06885895133018494\n",
            "epoch 23 , loss 0.0687645673751831\n",
            "epoch 24 , loss 0.06867089122533798\n",
            "epoch 25 , loss 0.06857790797948837\n",
            "epoch 26 , loss 0.06848561763763428\n",
            "epoch 27 , loss 0.0683940052986145\n",
            "epoch 28 , loss 0.06830312311649323\n",
            "epoch 29 , loss 0.06821286678314209\n",
            "epoch 30 , loss 0.06812328100204468\n",
            "epoch 31 , loss 0.06803436577320099\n",
            "epoch 32 , loss 0.06794608384370804\n",
            "epoch 33 , loss 0.06785845756530762\n",
            "epoch 34 , loss 0.06777147948741913\n",
            "epoch 35 , loss 0.0676850900053978\n",
            "epoch 36 , loss 0.06759937107563019\n",
            "epoch 37 , loss 0.06751423329114914\n",
            "epoch 38 , loss 0.06742971390485764\n",
            "epoch 39 , loss 0.06734581291675568\n",
            "epoch 40 , loss 0.06726250797510147\n",
            "epoch 41 , loss 0.06717975437641144\n",
            "epoch 42 , loss 0.06709762662649155\n",
            "epoch 43 , loss 0.06701603531837463\n",
            "epoch 44 , loss 0.06693503260612488\n",
            "epoch 45 , loss 0.06685459613800049\n",
            "epoch 46 , loss 0.06677471846342087\n",
            "epoch 47 , loss 0.06669536232948303\n",
            "epoch 48 , loss 0.06661657243967056\n",
            "epoch 49 , loss 0.06653834879398346\n",
            "epoch 50 , loss 0.06646063923835754\n",
            "epoch 51 , loss 0.06638345867395401\n",
            "epoch 52 , loss 0.06630677729845047\n",
            "epoch 53 , loss 0.0662306547164917\n",
            "epoch 54 , loss 0.06615502387285233\n",
            "epoch 55 , loss 0.06607991456985474\n",
            "epoch 56 , loss 0.06600527465343475\n",
            "epoch 57 , loss 0.06593115627765656\n",
            "epoch 58 , loss 0.06585752218961716\n",
            "epoch 59 , loss 0.06578437238931656\n",
            "epoch 60 , loss 0.06571169197559357\n",
            "epoch 61 , loss 0.06563950330018997\n",
            "epoch 62 , loss 0.06556778401136398\n",
            "epoch 63 , loss 0.0654965415596962\n",
            "epoch 64 , loss 0.06542575359344482\n",
            "epoch 65 , loss 0.06535540521144867\n",
            "epoch 66 , loss 0.06528552621603012\n",
            "epoch 67 , loss 0.06521609425544739\n",
            "epoch 68 , loss 0.06514711678028107\n",
            "epoch 69 , loss 0.06507853418588638\n",
            "epoch 70 , loss 0.0650104358792305\n",
            "epoch 71 , loss 0.06494275480508804\n",
            "epoch 72 , loss 0.06487547606229782\n",
            "epoch 73 , loss 0.06480865180492401\n",
            "epoch 74 , loss 0.06474220752716064\n",
            "epoch 75 , loss 0.0646762028336525\n",
            "epoch 76 , loss 0.06461060792207718\n",
            "epoch 77 , loss 0.06454542279243469\n",
            "epoch 78 , loss 0.06448061019182205\n",
            "epoch 79 , loss 0.06441622227430344\n",
            "epoch 80 , loss 0.06435222178697586\n",
            "epoch 81 , loss 0.06428860872983932\n",
            "epoch 82 , loss 0.06422538310289383\n",
            "epoch 83 , loss 0.06416250020265579\n",
            "epoch 84 , loss 0.06410004943609238\n",
            "epoch 85 , loss 0.06403794884681702\n",
            "epoch 86 , loss 0.06397625803947449\n",
            "epoch 87 , loss 0.06391486525535583\n",
            "epoch 88 , loss 0.06385387480258942\n",
            "epoch 89 , loss 0.06379324942827225\n",
            "epoch 90 , loss 0.06373297423124313\n",
            "epoch 91 , loss 0.06367306411266327\n",
            "epoch 92 , loss 0.06361350417137146\n",
            "epoch 93 , loss 0.06355426460504532\n",
            "epoch 94 , loss 0.06349538266658783\n",
            "epoch 95 , loss 0.0634368360042572\n",
            "epoch 96 , loss 0.06337863951921463\n",
            "epoch 97 , loss 0.06332073360681534\n",
            "epoch 98 , loss 0.06326321512460709\n",
            "epoch 99 , loss 0.06320600211620331\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = model(X_test).round()\n",
        "# .round() sets value to 0 or 1 on threshold"
      ],
      "metadata": {
        "id": "kOy3CUFHcJHy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sNAktrXUc3yS",
        "outputId": "0a8cd9e0-2412-4178-ebd3-d378f64a7ff6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1.],\n",
              "        [0.],\n",
              "        [0.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [0.],\n",
              "        [0.],\n",
              "        [0.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [0.],\n",
              "        [1.],\n",
              "        [0.],\n",
              "        [1.],\n",
              "        [0.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [0.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [0.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [0.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [0.],\n",
              "        [1.],\n",
              "        [0.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [0.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [0.],\n",
              "        [0.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [0.],\n",
              "        [0.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [0.],\n",
              "        [0.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [0.],\n",
              "        [0.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [0.],\n",
              "        [0.],\n",
              "        [1.],\n",
              "        [0.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [0.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [0.],\n",
              "        [1.],\n",
              "        [0.],\n",
              "        [0.],\n",
              "        [0.],\n",
              "        [0.],\n",
              "        [0.],\n",
              "        [0.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [0.],\n",
              "        [0.],\n",
              "        [1.],\n",
              "        [0.],\n",
              "        [0.],\n",
              "        [1.],\n",
              "        [0.],\n",
              "        [0.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [0.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [0.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [0.],\n",
              "        [0.],\n",
              "        [0.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [0.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [0.],\n",
              "        [1.],\n",
              "        [0.],\n",
              "        [0.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [0.],\n",
              "        [0.],\n",
              "        [0.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [0.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [0.],\n",
              "        [1.],\n",
              "        [0.],\n",
              "        [1.]], grad_fn=<RoundBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "acc = y_pred.eq(y_test).sum().item()/float(y_test.shape[0])\n",
        "acc"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mAkK2ueLdE10",
        "outputId": "471b2d71-288f-488a-b136-a4a6bb53af5a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.986013986013986"
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ej_WSPlTdYOs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Softmax"
      ],
      "metadata": {
        "id": "xHx16WDilER_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "arr = torch.tensor([7,12,1,4],dtype=torch.float32)"
      ],
      "metadata": {
        "id": "AEWQmAM8lj9x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.softmax(arr,dim=0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S2zQObuFlpNR",
        "outputId": "8ab34960-b255-4ece-fa83-5ba866cb4d8a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([6.6905e-03, 9.9296e-01, 1.6584e-05, 3.3310e-04])"
            ]
          },
          "metadata": {},
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.softmax(arr,dim=0).sum()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ylpqHi3VltMx",
        "outputId": "5e39a985-df3f-4502-e3b9-542523040eb9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(1.)"
            ]
          },
          "metadata": {},
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## cross entropy\n",
        "# no need to apply softmax on the las layer or change the actual values to one_hot_encoding"
      ],
      "metadata": {
        "id": "9tMNuJhjl16U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QJKiI2x9nSsc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}